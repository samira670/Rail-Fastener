# -*- coding: utf-8 -*-
"""ViT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v-PKv3_g0nsonEKN6eMrYfp_lhmW8UdC
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torchvision --upgrade

!pip install timm



!pip install transformers

# Importing necessary libraries and modules
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, random_split, ConcatDataset
from torchvision import datasets, transforms
import timm
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support

# Set the path to your dataset



# Set the device for training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the transforms for training, validation, and testing data
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'validation': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}


data_dir = '/content/drive/MyDrive/combined_dataset'

# Load the entire dataset
entire_dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])

# Calculate sizes for training, validation, and test splits
train_size = int(0.7 * len(entire_dataset))
validation_size = (len(entire_dataset) - train_size) // 2
test_size = len(entire_dataset) - train_size - validation_size

# Perform the random split
train_dataset, validation_dataset, test_dataset = random_split(entire_dataset, [train_size, validation_size, test_size])

# Update the transforms for validation and test datasets
validation_dataset.dataset.transform = data_transforms['validation']
test_dataset.dataset.transform = data_transforms['test']






# Get the number of classes in the dataset
num_classes = len(entire_dataset.classes)


# Create the model

def create_model():
    model = timm.create_model('vit_base_patch16_224', pretrained=True)
    num_features = model.head.in_features
    model.head = nn.Sequential(
        nn.Dropout(0.5), # Add Dropout layer
        nn.Linear(num_features, num_classes)
    )
    return model.to(device)




# Define the loss function

criterion = nn.CrossEntropyLoss()



def compute_metrics(targets, predictions):
    precision, recall, f1, _ = precision_recall_fscore_support(targets, predictions, average='weighted')
    return precision, recall, f1

# Train the model with KFold cross-validation

def kfold_train_model(num_epochs=8, k=10):
    all_train_losses = []
    all_train_accuracies = []
    all_val_losses = []
    all_val_accuracies = []
    all_train_precisions = []
    all_train_recalls = []
    all_train_f1s = []
    all_val_precisions = []
    all_val_recalls = []
    all_val_f1s = []


    # Concatenate training and validation sets
    full_dataset = ConcatDataset([train_dataset.dataset, validation_dataset.dataset])


    # KFold cross-validation
    num_samples = len(full_dataset)
    indices = list(range(num_samples))
    fold_len = num_samples // k
    np.random.shuffle(indices)

    for fold in range(k):
        train_losses = []
        train_accuracies = []
        train_precisions=[]
        train_recalls=[]
        train_f1s=[]
        val_losses = []
        val_accuracies = []
        val_precisions=[]
        val_recalls=[]
        val_f1s=[]









        print(f"Fold {fold+1}/{k}")
        print('-' * 10)

        val_indices = indices[fold*fold_len: (fold+1)*fold_len]
        train_indices = [idx for idx in indices if idx not in val_indices]

        train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)
        val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)

        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_sampler, num_workers=1)
        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_sampler, num_workers=1)

        model = create_model()

        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # Learning rate scheduler



        for epoch in range(num_epochs):
            # Train
            model.train()
            running_loss = 0.0
            running_corrects = 0
            all_train_preds = []
            all_train_labels = []

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                with torch.set_grad_enabled(True):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data).item()
                all_train_preds.extend(preds.cpu().numpy())
                all_train_labels.extend(labels.cpu().numpy())

            train_loss = running_loss / len(train_loader.sampler)
            train_acc = running_corrects / len(train_loader.sampler)
            train_precision, train_recall, train_f1 = compute_metrics(all_train_labels, all_train_preds)

            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
            train_precisions.append(train_precision)
            train_recalls.append(train_recall)
            train_f1s.append(train_f1)

            # Validation
            model.eval()
            running_loss = 0.0
            running_corrects = 0
            all_val_preds = []
            all_val_labels = []

            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                with torch.set_grad_enabled(False):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data).item()
                all_val_preds.extend(preds.cpu().numpy())
                all_val_labels.extend(labels.cpu().numpy())

            val_loss = running_loss / len(val_loader.sampler)
            scheduler.step(val_loss)
            val_acc = running_corrects / len(val_loader.sampler)
            val_precision, val_recall, val_f1 = compute_metrics(all_val_labels, all_val_preds)

            val_losses.append(val_loss)
            val_accuracies.append(val_acc)
            val_precisions.append(val_precision)
            val_recalls.append(val_recall)
            val_f1s.append(val_f1)

            print(f"Epoch {epoch+1}/{num_epochs} - "
                  f"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} Train Precision: {train_precision:.4f} "
                  f"Train Recall: {train_recall:.4f} Train F1: {train_f1:.4f} - "
                  f"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f} Val Precision: {val_precision:.4f} "
                  f"Val Recall: {val_recall:.4f} Val F1: {val_f1:.4f}")






        all_train_losses.append(train_losses)
        all_train_accuracies.append(train_accuracies)
        all_val_losses.append(val_losses)
        all_val_accuracies.append(val_accuracies)
        all_train_precisions.append(train_precisions)
        all_train_recalls.append(train_recalls)
        all_train_f1s.append(train_f1s)
        all_val_precisions.append(val_precisions)
        all_val_recalls.append(val_recalls)
        all_val_f1s.append(val_f1s)






        print('-' * 10)

    # Average loss and accuracy values across all folds
    avg_train_losses = np.mean(all_train_losses, axis=0)
    avg_train_accuracies = np.mean(all_train_accuracies, axis=0)
    avg_train_precisions = np.mean(all_train_precisions, axis=0)
    avg_train_recalls = np.mean(all_train_recalls, axis=0)
    avg_train_f1s= np.mean(all_train_f1s, axis=0)
    avg_val_losses = np.mean(all_val_losses, axis=0)
    avg_val_accuracies = np.mean(all_val_accuracies, axis=0)
    avg_val_precisions = np.mean(all_val_precisions, axis=0)
    avg_val_recalls = np.mean(all_val_recalls, axis=0)
    avg_val_f1s = np.mean(all_val_f1s, axis=0)




    # Plotting
    plt.figure(figsize=(20, 10))

    # Loss subplot
    plt.subplot(2, 3, 1)
    plt.plot(avg_train_losses, '-o', label='Training Loss')
    plt.plot(avg_val_losses, '-o', label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Average Training and Validation Loss across folds')

    # Accuracy subplot
    plt.subplot(2, 3, 2)
    plt.plot(avg_train_accuracies, '-o', label='Training Accuracy')
    plt.plot(avg_val_accuracies, '-o', label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Average Training and Validation Accuracy across folds')

    # Precision subplot
    plt.subplot(2, 3, 3)
    plt.plot(avg_train_precisions, '-o', label='Training Precision')
    plt.plot(avg_val_precisions, '-o', label='Validation Precision')
    plt.xlabel('Epochs')
    plt.ylabel('Precision')
    plt.legend()
    plt.title('Average Training and Validation Precision across folds')

    # Recall subplot
    plt.subplot(2, 3, 4)
    plt.plot(avg_train_recalls, '-o', label='Training Recall')
    plt.plot(avg_val_recalls, '-o', label='Validation Recall')
    plt.xlabel('Epochs')
    plt.ylabel('Recall')
    plt.legend()
    plt.title('Average Training and Validation Recall across folds')

    # F1 Score subplot
    plt.subplot(2, 3, 5)
    plt.plot(avg_train_f1s, '-o', label='Training F1 Score')
    plt.plot(avg_val_f1s, '-o', label='Validation F1 Score')
    plt.xlabel('Epochs')
    plt.ylabel('F1 Score')
    plt.legend()
    plt.title('Average Training and Validation F1 Score across folds')

    plt.tight_layout()
    plt.show()


    # Run KFold cross-validation
num_epochs = 10
kfold_train_model(num_epochs=num_epochs, k=10)

