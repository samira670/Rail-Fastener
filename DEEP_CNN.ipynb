{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWCymE0_1m7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7f15c5-01ba-4fa5-e1e3-59578b2853c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Dataset\" \"/content/\"\n"
      ],
      "metadata": {
        "id": "qw1xi5W-6Bvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PR1oT5tU3a-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f3595d-e10f-423a-f5fc-02cd60af7fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.2 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNP-qx1IcjE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8924ec9-3956-4ca9-d25c-73be84424d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, transformers\n",
            "Successfully installed safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTzw4VtDVTdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5fe3c6-6b42-46ff-96b1-81108d12902d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n"
      ],
      "metadata": {
        "id": "TTLx8xd4kfAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab63e0e-3aa3-4b02-9ad7-bb5a7b0b9a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl-XkNw_1jGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ba406a-36a9-43d4-9153-870a4f6e9742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 1\n",
            "Epoch [1/10] Training Loss: 0.0249, Training Accuracy: 0.5267, Training Precision: 0.5267, Training Recall: 0.5267, Training F1: 0.5265, Validation Loss: 0.0235, Validation Accuracy: 0.4667, Validation Precision: 0.7511, Validation Recall: 0.4667, Validation F1: 0.2970\n",
            "Epoch [2/10] Training Loss: 0.0218, Training Accuracy: 0.5641, Training Precision: 0.5643, Training Recall: 0.5641, Training F1: 0.5639, Validation Loss: 0.0226, Validation Accuracy: 0.5143, Validation Precision: 0.4718, Validation Recall: 0.5143, Validation F1: 0.4220\n",
            "Epoch [3/10] Training Loss: 0.0216, Training Accuracy: 0.5551, Training Precision: 0.5551, Training Recall: 0.5551, Training F1: 0.5549, Validation Loss: 0.0221, Validation Accuracy: 0.6238, Validation Precision: 0.6334, Validation Recall: 0.6238, Validation F1: 0.6050\n",
            "Epoch [4/10] Training Loss: 0.0205, Training Accuracy: 0.6481, Training Precision: 0.6481, Training Recall: 0.6481, Training F1: 0.6481, Validation Loss: 0.0215, Validation Accuracy: 0.6714, Validation Precision: 0.6752, Validation Recall: 0.6714, Validation F1: 0.6649\n",
            "Epoch [5/10] Training Loss: 0.0197, Training Accuracy: 0.6788, Training Precision: 0.6792, Training Recall: 0.6788, Training F1: 0.6786, Validation Loss: 0.0224, Validation Accuracy: 0.5429, Validation Precision: 0.5458, Validation Recall: 0.5429, Validation F1: 0.4508\n",
            "Epoch [6/10] Training Loss: 0.0190, Training Accuracy: 0.6867, Training Precision: 0.7067, Training Recall: 0.6867, Training F1: 0.6787, Validation Loss: 0.0210, Validation Accuracy: 0.7048, Validation Precision: 0.7097, Validation Recall: 0.7048, Validation F1: 0.7049\n",
            "Epoch [7/10] Training Loss: 0.0180, Training Accuracy: 0.7696, Training Precision: 0.7753, Training Recall: 0.7696, Training F1: 0.7685, Validation Loss: 0.0207, Validation Accuracy: 0.6762, Validation Precision: 0.6770, Validation Recall: 0.6762, Validation F1: 0.6726\n",
            "Epoch [8/10] Training Loss: 0.0178, Training Accuracy: 0.7594, Training Precision: 0.7599, Training Recall: 0.7594, Training F1: 0.7592, Validation Loss: 0.0206, Validation Accuracy: 0.7048, Validation Precision: 0.7097, Validation Recall: 0.7048, Validation F1: 0.7049\n",
            "Epoch [9/10] Training Loss: 0.0176, Training Accuracy: 0.7673, Training Precision: 0.7684, Training Recall: 0.7673, Training F1: 0.7671, Validation Loss: 0.0205, Validation Accuracy: 0.6714, Validation Precision: 0.6707, Validation Recall: 0.6714, Validation F1: 0.6703\n",
            "Epoch [10/10] Training Loss: 0.0174, Training Accuracy: 0.7787, Training Precision: 0.7796, Training Recall: 0.7787, Training F1: 0.7785, Validation Loss: 0.0204, Validation Accuracy: 0.6810, Validation Precision: 0.6842, Validation Recall: 0.6810, Validation F1: 0.6755\n",
            "FOLD 2\n",
            "Epoch [1/10] Training Loss: 0.0301, Training Accuracy: 0.4983, Training Precision: 0.4981, Training Recall: 0.4983, Training F1: 0.4982, Validation Loss: 0.0229, Validation Accuracy: 0.5333, Validation Precision: 0.7511, Validation Recall: 0.5333, Validation F1: 0.3710\n",
            "Epoch [2/10] Training Loss: 0.0224, Training Accuracy: 0.5346, Training Precision: 0.5338, Training Recall: 0.5346, Training F1: 0.5336, Validation Loss: 0.0225, Validation Accuracy: 0.5714, Validation Precision: 0.7059, Validation Recall: 0.5714, Validation F1: 0.5109\n",
            "Epoch [3/10] Training Loss: 0.0206, Training Accuracy: 0.6254, Training Precision: 0.6252, Training Recall: 0.6254, Training F1: 0.6252, Validation Loss: 0.0223, Validation Accuracy: 0.5381, Validation Precision: 0.5310, Validation Recall: 0.5381, Validation F1: 0.4578\n",
            "Epoch [4/10] Training Loss: 0.0205, Training Accuracy: 0.6175, Training Precision: 0.6182, Training Recall: 0.6175, Training F1: 0.6176, Validation Loss: 0.0228, Validation Accuracy: 0.5238, Validation Precision: 0.2821, Validation Recall: 0.5238, Validation F1: 0.3667\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import (ToTensor, Resize, RandomResizedCrop, RandomRotation,\n",
        "                                   RandomHorizontalFlip, RandomVerticalFlip, ColorJitter)\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "!cp -r \"/content/drive/MyDrive/combined_dataset\" \"/content/combined_dataset\"\n",
        "\n",
        "\n",
        "# Path to your dataset\n",
        "data_dir = '/content/drive/MyDrive/combined_dataset'\n",
        "\n",
        "# Parameters\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "num_folds = 10\n",
        "\n",
        "# Transforms\n",
        "train_transform = torchvision.transforms.Compose([\n",
        "    RandomResizedCrop(image_size),\n",
        "    RandomRotation(15),\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomVerticalFlip(),\n",
        "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "val_test_transform = torchvision.transforms.Compose([\n",
        "    Resize(image_size, antialias=True),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "# Loading the full dataset with train transform\n",
        "full_dataset = ImageFolder(data_dir, transform=train_transform)\n",
        "num_classes = len(full_dataset.classes)\n",
        "\n",
        "# Splitting the dataset into training, validation, and test sets (let's say 70%, 15%, 15%)\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "validation_size = (len(full_dataset) - train_size) // 2\n",
        "test_size = len(full_dataset) - train_size - validation_size\n",
        "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "# Apply the validation and test transforms to the respective splits\n",
        "validation_dataset.dataset.transform = val_test_transform\n",
        "test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(full_dataset.classes)\n",
        "\n",
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DeepCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 56 * 56, 512),  # 56x56 is the size after two max pooling operations on a 224x224 image\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_ids, _) in enumerate(kfold.split(train_dataset)):\n",
        "    print(f\"FOLD {fold + 1}\")\n",
        "\n",
        "    train_subsampler = Subset(train_dataset, train_ids)\n",
        "    train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    model = DeepCNN(num_classes)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    train_acc_list, val_acc_list, train_loss_list, val_loss_list = [], [], [], []\n",
        "    train_prec_list, train_recall_list, train_f1_list = [], [], []\n",
        "    val_prec_list, val_recall_list, val_f1_list = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0.0, 0\n",
        "        all_labels, all_preds = [], []\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "        avg_train_accuracy = train_correct / len(train_loader.dataset)\n",
        "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=1)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0.0, 0\n",
        "        all_labels, all_preds = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "        avg_val_accuracy = val_correct / len(val_loader.dataset)\n",
        "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=1)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_accuracy:.4f}, Training Precision: {train_precision:.4f}, Training Recall: {train_recall:.4f}, Training F1: {train_f1:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}, Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1: {val_f1:.4f}\")\n",
        "\n",
        "        train_acc_list.append(avg_train_accuracy)\n",
        "        val_acc_list.append(avg_val_accuracy)\n",
        "        train_loss_list.append(avg_train_loss)\n",
        "        val_loss_list.append(avg_val_loss)\n",
        "        train_prec_list.append(train_precision)\n",
        "        train_recall_list.append(train_recall)\n",
        "        train_f1_list.append(train_f1)\n",
        "        val_prec_list.append(val_precision)\n",
        "        val_recall_list.append(val_recall)\n",
        "        val_f1_list.append(val_f1)\n",
        "\n",
        "    fold_results.append({\n",
        "        'train_acc': train_acc_list,\n",
        "        'val_acc': val_acc_list,\n",
        "        'train_loss': train_loss_list,\n",
        "        'val_loss': val_loss_list,\n",
        "        'train_prec': train_prec_list,\n",
        "        'val_prec': val_prec_list,\n",
        "        'train_recall': train_recall_list,\n",
        "        'val_recall': val_recall_list,\n",
        "        'train_f1': train_f1_list,\n",
        "        'val_f1': val_f1_list\n",
        "    })\n",
        "\n",
        "# Compute averages over all folds\n",
        "avg_train_acc = np.mean([result['train_acc'] for result in fold_results], axis=0)\n",
        "avg_val_acc = np.mean([result['val_acc'] for result in fold_results], axis=0)\n",
        "avg_train_loss = np.mean([result['train_loss'] for result in fold_results], axis=0)\n",
        "avg_val_loss = np.mean([result['val_loss'] for result in fold_results], axis=0)\n",
        "avg_train_prec = np.mean([result['train_prec'] for result in fold_results], axis=0)\n",
        "avg_val_prec = np.mean([result['val_prec'] for result in fold_results], axis=0)\n",
        "avg_train_recall = np.mean([result['train_recall'] for result in fold_results], axis=0)\n",
        "avg_val_recall = np.mean([result['val_recall'] for result in fold_results], axis=0)\n",
        "avg_train_f1 = np.mean([result['train_f1'] for result in fold_results], axis=0)\n",
        "avg_val_f1 = np.mean([result['val_f1'] for result in fold_results], axis=0)\n",
        "\n",
        "# Plotting the average metrics\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(range(num_epochs), avg_train_acc, label='Training')\n",
        "plt.plot(range(num_epochs), avg_val_acc, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(range(num_epochs), avg_train_loss, label='Training')\n",
        "plt.plot(range(num_epochs), avg_val_loss, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(range(num_epochs), avg_train_prec, label='Training')\n",
        "plt.plot(range(num_epochs), avg_val_prec, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(range(num_epochs), avg_train_recall, label='Training')\n",
        "plt.plot(range(num_epochs), avg_val_recall, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdBB4FD0SL7D"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to store values\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    \n",
        "    # Training code...\n",
        "\n",
        "    # Calculate train accuracy and append to list\n",
        "    train_accuracy = train_correct / len(train_dataset)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    \n",
        "    # Testing code...\n",
        "    \n",
        "    # Calculate test accuracy and append to list\n",
        "    test_accuracy = test_correct / len(test_dataset)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Append train and test losses to lists\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "# Plotting the tables\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Plot train and test losses\n",
        "plt.plot(epochs, train_losses, label='Train Loss')\n",
        "plt.plot(epochs, test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Test Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot train and test accuracies\n",
        "plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}